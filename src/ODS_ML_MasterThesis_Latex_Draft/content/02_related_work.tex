\chapter{Related Work}
\label{chapter:related_work}

We will now explore previous work that is relevant to ours and discuss how it influenced, inspired, or varied from this present endeavor. 

% inspired from Leon
\section{The Heavy Hitter Problem and Count-Min Sketch}

% copy from https://datagenetics.com/blog/november22019/index.html
Heavy Hitter~\cite{sivaraman2017heavy} is a common problem in computer science. For instance, there could be logs of search terms, and you are trying to find the most commonly searched query. Maybe there are error logs files, and you are trying to find the most common error, or possibly IP addresses of incoming requests, and you want to know who is using your service this most.

An elegant small-space data structure, the count-min sketch~\cite{cormode2005improved}, can be used to solve the heavy hitter problem. It has been implemented in real systems. The data structure is based on hashing. Hashing-based algorithms have several useful properties. In particular, they can handle item deletions, which are implemented by decrementing the respective counters. Furthermore, Count-Min never underestimate the true frequencies. However, Count-Min algorithms lead to estimation errors due to collisions: when two elements are mapped to the same bucket, they affect each other's estimates. As a result, some items remain indistinguishable and optimal design is necessary, otherwise the rough upper bound estimation could lead to significant errors. By leveraging useful patterns in the input data, we can improve the frequency output and reduce errors. Deep learning models are highly successful at capturing and utilizing complex data patterns, and hence needed to be integrated into count-min sketch.


\section{Adaptive Learning Algorithm}

% paraphrase A survey on concept drift adaptation. ACM Computing Surveys (CSUR) and add some my own portion
Often, learning algorithms must function in dynamic contexts that change unexpectedly. One of these algorithms' attractive properties is their capacity to incorporate fresh data. If the data generation process is not absolutely stationary (which is the case for the vast majority of real-world applications), the underlying notion being predicted may change over time. Adaptive Learning~\cite{botu2015adaptive}, as its name says, can quickly adapt to new information and gain insight into how important that new information is.

Adapting to such concept drift may be viewed as a logical extension of incremental learning systems~\cite{gepperth2016incremental} that build predictive models~\cite{candanedo2018machine} example by example. Adaptive learning algorithms can be thought of as sophisticated incremental learning algorithms that can adapt to new behavior instantly.

In production, nevertheless, ground truth computation is incredibly expensive. In addition, labels for latest data could not be accessible. To optimize computation resources and improve training performance, we incorporate semi-supervised learning into the scheme. By applying Sketches to the predictions, we acquire pseudo-labels that can be used for continue training.

\section{Learned Sketch}

Learned Sketch is a learning-based frequency estimation streaming algorithm. The technique~\cite{hsu2019learning} has been proposed with a learning model that enables them to exploit data properties without being restricted to a particular pattern or having prior knowledge of the desirable property. 

The system makes use of an oracle to evaluate if an item is a "big hitter". All objects identified as heavy hitters are assigned to unique buckets. The remaining items are allocated to remaining buckets using a standard frequency estimating method. 

However, Learned Sketch is only a static model: there is no mechanism to adapt to temporal patterns. In comparison with Learned Sketch, we bring the online ability~\cite{benczur2018online} to the table. 


\section{Hybridized classification}

% paraphrase from  https://www.sciencedirect.com/science/article/pii/S1110866520301377
Machine Learning (ML) is a subfield of Artificial Intelligence (AI) concerned with the conversion of raw data to meaningful information. With the exponential growth in the quantity of data accessible today, it is necessary to develop and apply data classifying approaches that are compatible and capable of addressing and facilitating effective and competent data analysis. Today, the majority of sophisticated classification approaches are integrated or hybridized with optimization techniques in order to improve classification accuracy and minimize processing time for such classification systems. 

Since we plan to inspect a fast-paced data stream and have a very large amount of data to process, ML hybridization can definitely be deployed in our system.
% Top-backpropagated vs. General 
% In Learned Sketch, the latent representation attempts to learn the whole distribution in an undifferentiated manner. In contrast, we make the latent representation train from
% optimized blend of latent and explicit representation?

\section{Sequence encoding and online tracking}

Data is passed unidirectionally in recurrent neural networks. Each neuron conveys information to the next neuron and back to itself. Long short-term memory (LSTM)~\cite{yu2019review} is a sort of recurrent neural network that is distinct from other types. It is capable of retaining a state or memory and acquiring knowledge about long-term patterns. Due to its state-keeping and recurrence properties, LSTMs are excellent candidates for sequential pattern encoding and for online settings, since they effectively handle concept drift and adapt to changes in the patterns. Recent studies have built an LSTM-based model~\cite{hsu2019learning} for detecting heavy hitters. 

A significant drawback of LSTM models is their omission of temporal patterns. They treat the sequence of events as though they occurred without regard for time. Unfortunately, this is an inaccurate characterization of reality. Latest improvements feature the design of time-aware LSTMs, named T-LSTMs~\cite{mou2019t}. When timestamp information is incorporated, the model performance can remarkably increase.

Our model follows the same ideas. The timestamps of our datasets are provided. By nesting the lstm-based network inside the bigger t-lstm framework, we can foresee a comparable performance boost. 













