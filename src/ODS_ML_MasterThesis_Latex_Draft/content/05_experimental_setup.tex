\chapter{Experimental Setup}
\label{chapter:experimental_setup}

We will present the experiments we conducted in this chapter, as well as the metrics and datasets we utilized. 

\section{Code Base}

\paragraph{Upgrading existing Code Base to TensorFlow 2}

We use as a starting code basis the code from Learned Sketch~\cite{hsu2019learning}.

% https://towardsdatascience.com/everything-you-need-to-know-about-tensorflow-2-0-b0856960c074
TensorFlow~\cite{tensorflow2015-whitepaper} is a general-purpose high-performance computing library open-sourced by Google in 2015. Since the beginning, its main focus was to provide high-performance APIs for building Neural Networks (NNs). However, with the advance of time and interest by the Machine Learning (ML) community, the library has grown into a full ML ecosystem.

Compared to TensorFlow 1.0, TensorFlow 2.0 focuses on simplicity and ease of use, with updates like eager execution, intuitive higher-level APIs, and flexible model building on any platform. There are multiple changes in TensorFlow 2.0 to make TensorFlow users more productive.

% from TensorFlow 1.0 to TensorFlow 2.0
\paragraph{Full Use of Modern TensorFlow Utilities}
In order to be able to transfer feedback conveniently, we upgraded the entirety of the Learned Sketch code from TensorFlow 1.0 to TensorFlow 2.0 and added some other improvements: 1)refactor the code into smaller functions 2)use Keras layers and models to manage variables 3)combine tf.data.Datasets and tf.function 4)take advantage of AutoGraph with Python control flow 5)tf.metrics aggregates data and tf.summary logs them 
% https://medium.com/featurepreneur/tensorflow-1-0-vs-tensorflow-2-0-240fc6efb14f

\paragraph{Functional API}
The Keras functional API is more flexible than the Sequential API in terms of model definition.
It allows us to define models with multiple inputs or outputs, as well as models with shared layers.
Additionally, it enables the development of ad hoc acyclic network graphs.
Models are built by constructing instances of layers and linking them in pairs, followed by defining a model that indicates the layers that will function as the model's input and output. 

\section{Dataset}

\paragraph{Source Data}
The traffic data is collected at a backbone link of a Tier1 ISP between Chicago and Seattle in 2016 (CAIDA). Each recording session is around one hour. Within each minute, there are around 30 million packets and 1 million unique flows. Counting them all, one hour of traffic involving about 2 billion IPv4 packets was evaluated (128 GB compressed, 264 GB uncompressed). 

\paragraph{Preprocessing}
We preprocessed the features and counts of each unique internet flow in each minute. To rearrange raw data in chronological-window order, we divided the data stream into fixed-length windows. The packets were aggregated into \emph{flows} for each window: Two packets were included in the same flow if they had the same source and destination IP addresses and ports number, as well as the transport protocol, according to the standard networking definitions. 

Additionally, the number of packets associated with each flow was stored as meta information for each flow. The table~\ref{tab:dataset_stats} below summarizes some statistics for various window lengths. 

However, the traffic was not consistently collected: each one-minute time frame has several small intervals(up to 6 seconds) during which no packets are collected. To mitigate this problem, windows that did not surpass a packet count threshold were filtered away. (This problem harmed our capacity to detect concept drift, implying that our results might be marginally better in a real-world production situation with a more reliable capturing system.) 

\begin{table}[htbp!]
\centering
\begin{tabular}{|r|r|r|r|}
\hline
\textbf{window length in $s$} & \textbf{number of w.} & \textbf{avg. packets per w.} & \textbf{avg. flows per w.} \\ \hline
 0.1   & 34,526 &    53,000 &  14,700 \\ 
 1.0   &  3,508 &   527,000 &  66,000 \\ 
10.0   &    360 & 5,100,000 & 280,000 \\ \hline
\end{tabular}
\caption{Statistics for different window lengths}
\label{tab:dataset_stats}
\end{table}

\paragraph{Data Partition}
To optimize the model in a time-aware manner, we split our dataset. We include 20 consecutive windows in our dataset so that it covers a range of temporal period. 
The latest windows in the sequence will be the validation and the test set, the earlier ones will be the training set for supervised learning or semi-supervised learning.
The goal is for our final system to predict as precisely as possible for each sequence the most recent elements in the sequence, the test set. 

\section{Structure of ML model}
It is not only neural network layers that we must consider while constructing machine learning models; there are other factors to take into account. In order to improve the performance, we must also pay attention to aspects such as model input/output, choice of activation function, overfitting, and tuning parameters etc. 

\paragraph{Encoding}
The model takes as input the IP addresses and reshape them to fit in the Keras network. We use two RNNs which consist of LSTM cells to encode the source and destination IP addresses separately. The RNN takes one bit of the IP address at each step, starting from the most significant bit. We use the final states of the RNN as the feature vector for an IP address. The reason to use RNN is that the patterns in the bits are hierarchical, i.e., the more significant bits govern larger regions in the IP space.

The items are flows and when packets are seen as belong to a flow they update the attributes of that flow. I.e., the items gets updated.

\paragraph{Dropout}
Simply expressed, dropout refers to the practice of ignoring units (i.e. neurons) during the training phase of a randomly chosen group of neurons. These units are discarded during a forward or backward pass.
To put it another way, at each training step, individual nodes are either eliminated from the network with probability 1-p or retained with probability p, resulting in a reduced network; incoming and outgoing edges to a dropped-out node are likewise removed. 
Dropout is used to alleviate over-fitting. Because a fully connected layer takes the majority of the parameters, neurons develop co-dependence upon each other during training, limiting each neuron's individual function and resulting in over-fitting of training data. 


\paragraph{Multi-output}
We do not only care about predicted counts which can be interpreted as regression as well, we also want our model to perform well in classification. Therefore, we choose muti-output model structure including both of regression and classification output. 
We add a sigmoid activation layer on top of the regression. All sigmoid functions have the property that they map the entire number line into a small range such as between 0 and 1, or -1 and 1, so we make use of a sigmoid function to convert a count, being a real value, into one that can be interpreted as a probability.
By doing so, we obtain regression output as well as classification output.

\paragraph{Pooling the top-k}
To give the system an incentive to get better on the items that really matter, on top of the regression (old Learned Oracle neural network), we added sorting \& classification  (parametrized only by k, so no weights here) into the model to compute the top F1. Then, we would be able to use as loss weighted sum of top F1 and MSE, possibly with automated meta optimization. 


\section{Model Training and Evaluation}
\subsection{Loss Functions}

\paragraph{Mean squared error} 
Mean squared error (MSE) is the most commonly used loss function for regression. In our case, it is used for predicted counts. The MSE is great for ensuring that our trained model has no outlier predictions with huge errors.

\paragraph{Cross-entropy loss} 
We choose Categorical crossentropy as our loss function for classification output, presented as probabilities. For each item, we have two classes: top and bottom, each of them has a prediction score indicating the possibility of belonging to that class. We provide labels in a one-hot representation. 

\paragraph{Class weight}
The occurrence of the bottom class is very high compared to the top class, that is, the target classâ€™s frequency is highly imbalanced. We modify the current training algorithm to take into account the skewed distribution of the classes. We give different weights to both the majority and minority classes. By setting class weight, we tell the model to "pay more attention" to samples from an under-represented class.
% assign values later

\subsection{Metrics}

\paragraph{Mean Absolute Error} 
Mean Absolute Error (MAE) is a metric used to evaluate a Regression Model. This metric tells us how accurate our count predictions are and, what is the amount of deviation from the actual values.

\paragraph{AUC} 
We choose AUC as the metric for classification because AUC gives an assessment of performance across all possible tops, i.e., across all values of k.


\subsection{Other tuning parameters}
\paragraph{Batch size}
Since per window files contains 1000 flows, batch size must be a factor of 1000, that is: 100, 200, 400, etc. After careful observation, we decide to use a batch size of 200 or 500.

\paragraph{Adam Optimizer}
The results of the Adam optimizer are generally better than every other optimization algorithms, have faster computation time, and require fewer parameters for tuning. Because of all that, Adam is used as the default optimizer for our system.

\paragraph{Number of epochs}
After analyzing the convergence of model performance over a range of epoch values, we conclude that 10 or 20 could be the optimal value for our system. 

\section{Count-min Sketch parameters}
Count-min comes into play when we apply semi-supervised learning. Therefore, we also need to set parameters of count-min. 

We determine a value of 2 for the number of hash functions.

The space includes space for storing the buckets and the model. Due to the fact that we employ the same model throughout the 50-minute testing period, the model space is amortized. We account for the extra space needed for the unique buckets to store the item IDs and the counts. One unique bucket takes 8 bytes, twice the space of a normal bucket. We choose a space value of 0.5 MB.

So far, we have:
\[ \text{space} = 0.5 \]
\[ n\_\text{hash} = 2 \]
\[ \text{cutoff}\_\text{cost}\_\text{mul} = 2 \]

Then, we apply above values to following formulas:
\[ \text{max}\_\text{bcut} = \text{space} * 1e3 / (4 * \text{cutoff}\_\text{cost}\_\text{mul}) \]
\[ \text{bcut} = 0.1 * \text{max}\_\text{bcut} \]
\[ n\_\text{cmin}\_\text{buckets} = (\text{space} * 1e3 - \text{bcut} * 4 * \text{cutoff}\_\text{cost}\_\text{mul}) / (n\_\text{hash} * 4) \]
\[ n\_\text{buckets} = \text{bcut} + n\_\text{cmin}\_\text{buckets} \]

Finally, we obtain the values for number of buckets.

\section{Top k pyramid parameters}

In the primary model, we utilize {128,32,32,1} as values of dense layer units to gradually get the count prediction. However, when implementing top k pyramid pooling, since we need to align the dimension values for sorting, assigning and overwriting, instead, we use {(128,32), (32,1), (32,1), (32,1), 1} as dense layers values.

As for values of k which directly concerns the degree of pooling, we use 500, 200 and 100 respectively in three  iterations.

\section{Experimentation Machine}
The machine we used for the experiments is MacBook Pro (13-inch, 2017) and is equipped with 3.1 GHz Dual-Core Intel Core i5  processor and 8 GB of RAM. 